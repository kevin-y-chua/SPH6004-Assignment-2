{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "great-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defined-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Time', 'stay_id', 'stay_key', 'hadm_id', 'age', 'gender', 'Heart Rate',\n",
    "       'Respiratory Rate', 'SpO2/SaO2', 'pH', 'Potassium', 'Calcium',\n",
    "       'Glucose', 'Sodium', 'HCO3', 'White Blood Cells', 'Hemoglobin',\n",
    "       'Red Blood Cells', 'Platelet Count', 'Weight', 'Urea Nitrogen',\n",
    "       'Creatinine', 'Blood Pressure', '1 hours urine output',\n",
    "       '6 hours urine output', 'AKI', 'gcs',\n",
    "       'ventilation', 'vasoactive medications', 'sedative medications']\n",
    "features = ['time_since', 'age', 'gender', 'Heart Rate',\n",
    "       'Respiratory Rate', 'SpO2/SaO2', 'pH', 'Potassium', 'Calcium',\n",
    "       'Glucose', 'Sodium', 'HCO3', 'White Blood Cells', 'Hemoglobin',\n",
    "       'Red Blood Cells', 'Platelet Count', 'Weight', 'Urea Nitrogen',\n",
    "       'Creatinine', 'Blood Pressure', '1 hours urine output',\n",
    "       '6 hours urine output', 'gcs',\n",
    "       'ventilation', 'vasoactive medications', 'sedative medications']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "authorized-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AKIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        self.dataframe = pd.read_csv(csv_file)\n",
    "        self.dataframe['stay_key'] = self.dataframe['stay_id']\n",
    "        self.dataframe = self.dataframe.groupby('stay_id')[cols].ffill().bfill()\n",
    "        in_time = self.dataframe.groupby('stay_key')[['Time']].first()\n",
    "        self.dataframe = pd.merge(self.dataframe, in_time, left_on=['stay_key'], right_index=True, how='left')\n",
    "        self.dataframe['time_since'] = (pd.to_datetime(self.dataframe['Time_x']) - pd.to_datetime(self.dataframe['Time_y'])) / np.timedelta64(1, 'h')\n",
    "        self.stay_ids = self.dataframe.stay_key.unique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stay_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = self.dataframe[self.dataframe.stay_key == self.stay_ids[idx]][features].to_numpy()\n",
    "        label = self.dataframe[self.dataframe.stay_key == self.stay_ids[idx]]['AKI'].to_numpy()\n",
    "        \n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "equal-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = AKIDataset('C:/Users/Kevin/Desktop/SPH6004 Assignment 2/assignment2_data/mimiciv_aki/time_series.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mighty-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notebook is currently set to only include the first 1000 unique stay_ids. Set to \"len(ds)\" to include all 39,742 stay IDs. Very slow though!!!\n",
    "n_stay_id = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "complicated-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Han's function returns a tuple. Separate the tuple into x (features) and y (outcomes).\n",
    "## x is a list \"n_stay_id\"s long. Each entry in the list contains a 2D array of shape (timesteps, features).\n",
    "## y is a list \"n_stay_id\"s long. Each entry in the list contains a 1D array of shape (timesteps)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for i in range(n_stay_id):\n",
    "    temp_x, temp_y = ds[i]\n",
    "    x.append(temp_x)\n",
    "    y.append(temp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "center-prisoner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "The maximum number of timesteps in the dataset is 687.0 steps.\n"
     ]
    }
   ],
   "source": [
    "## Extract the number of timesteps in x for each stay_id.\n",
    "\n",
    "x_timesteps = np.zeros(shape=(n_stay_id))\n",
    "\n",
    "for i in range(n_stay_id):\n",
    "    x_timesteps[i] = x[i].shape[0]\n",
    "     \n",
    "print(x_timesteps.shape)    \n",
    "print('The maximum number of timesteps in the dataset is', x_timesteps.max(), 'steps.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "infectious-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually pad the arrays for each stay_id in x and y to the maximum number of timesteps in the dataset. Pads with 0's.\n",
    "## Returns x_padded and y_padded.\n",
    "\n",
    "max_timesteps = int(x_timesteps.max())\n",
    "\n",
    "x_padded = []\n",
    "for i in range(n_stay_id):\n",
    "    temp_padded = np.pad(x[i], pad_width=((0, (max_timesteps-x[i].shape[0])), (0, 0)), mode='constant', constant_values=0)\n",
    "    x_padded.append(temp_padded)\n",
    "    \n",
    "y_padded = []\n",
    "for i in range(n_stay_id):\n",
    "    temp_padded = np.pad(y[i], pad_width=(0, max_timesteps-y[i].shape[0]), mode='constant', constant_values=0)\n",
    "    y_padded.append(temp_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "allied-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shift y-values (AKI status) one step backward. So that the features at time t predict the AKI values at time t+1.\n",
    "\n",
    "y_padded_shifted = []\n",
    "for i in range(n_stay_id):\n",
    "    next_y = shift(y_padded[i], -1, mode='constant', cval=0)\n",
    "    y_padded_shifted.append(next_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "backed-glucose",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(687,)\n",
      "(687,)\n"
     ]
    }
   ],
   "source": [
    "print(y_padded[3].shape)\n",
    "print(y_padded_shifted[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ambient-print",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 687, 26)\n",
      "(1000, 687)\n"
     ]
    }
   ],
   "source": [
    "## Convert the x and y lists into arrays to feed into the LSTM.\n",
    "## X is a 3D array of shape (n_stay_id, timesteps, features)\n",
    "## Y is a 2D array of shape (n_stay_id, timesteps)\n",
    "\n",
    "X = np.asarray(x_padded)\n",
    "Y = np.asarray(y_padded_shifted)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bridal-negative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 687, 26)]         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 687, 512)          1103872   \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 687, 256)          131328    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 687, 64)           16448     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 687, 1)            65        \n",
      "=================================================================\n",
      "Total params: 1,251,713\n",
      "Trainable params: 1,251,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Create a small neural network with an LSTM layer, that feeds into 2 fully-connected hidden layers. Output is a sigmoid activation function representing the probably of AKI at time t.\n",
    "\n",
    "X_inputs = keras.Input(shape=(X.shape[1], X.shape[2]))\n",
    "\n",
    "X_next = layers.LSTM(512, return_sequences=True)(X_inputs)\n",
    "X_next = layers.TimeDistributed(layers.Dense(256, activation='relu', kernel_regularizer='l2'))(X_next)\n",
    "X_next = layers.TimeDistributed(layers.Dense(64, activation='relu', kernel_regularizer='l2'))(X_next)                                \n",
    "output = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(X_next)  \n",
    "\n",
    "model = keras.Model(inputs=X_inputs, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ordered-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define optimizer, loss function, and evaluation metrics.\n",
    "\n",
    "adam = keras.optimizers.Adam()\n",
    "\n",
    "binarycrossentropy = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "binary_accuracy = keras.metrics.BinaryAccuracy()\n",
    "AUC = keras.metrics.AUC() # Need AUC because accuracy is a poor measure in this case.\n",
    "\n",
    "model.compile(optimizer=adam, loss=binarycrossentropy, metrics=[binary_accuracy, AUC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "scientific-lease",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 6s 224ms/step - loss: 3.2638 - binary_accuracy: 0.9889 - auc: 0.7534 - val_loss: 2.0003 - val_binary_accuracy: 0.9979 - val_auc: 0.9720\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 9s 347ms/step - loss: 1.3432 - binary_accuracy: 0.9972 - auc: 0.9688 - val_loss: 0.8021 - val_binary_accuracy: 0.9979 - val_auc: 0.9682\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 4s 148ms/step - loss: 0.5321 - binary_accuracy: 0.9972 - auc: 0.9740 - val_loss: 0.3172 - val_binary_accuracy: 0.9979 - val_auc: 0.9084\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 3s 139ms/step - loss: 0.2145 - binary_accuracy: 0.9972 - auc: 0.9708 - val_loss: 0.1321 - val_binary_accuracy: 0.9979 - val_auc: 0.9702\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 3s 138ms/step - loss: 0.0946 - binary_accuracy: 0.9972 - auc: 0.9721 - val_loss: 0.0628 - val_binary_accuracy: 0.9979 - val_auc: 0.9724\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 3s 139ms/step - loss: 0.0488 - binary_accuracy: 0.9972 - auc: 0.9767 - val_loss: 0.0358 - val_binary_accuracy: 0.9979 - val_auc: 0.9484\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 3s 139ms/step - loss: 0.0308 - binary_accuracy: 0.9972 - auc: 0.9687 - val_loss: 0.0244 - val_binary_accuracy: 0.9979 - val_auc: 0.9680\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 3s 138ms/step - loss: 0.0230 - binary_accuracy: 0.9972 - auc: 0.9607 - val_loss: 0.0193 - val_binary_accuracy: 0.9979 - val_auc: 0.9736\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 3s 139ms/step - loss: 0.0192 - binary_accuracy: 0.9972 - auc: 0.9728 - val_loss: 0.0173 - val_binary_accuracy: 0.9979 - val_auc: 0.9373\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 3s 138ms/step - loss: 0.0178 - binary_accuracy: 0.9972 - auc: 0.9627 - val_loss: 0.0165 - val_binary_accuracy: 0.9979 - val_auc: 0.9192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24bd378a8e0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fit and evaluate model with cross-validated metrics.\n",
    "\n",
    "model.fit(x=X, y=Y, epochs=10, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-agency",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
